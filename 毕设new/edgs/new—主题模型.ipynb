{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim实现抽取文档主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"改革\" + 0.015*\"农业\" + 0.014*\"实践\" + 0.014*\"水价\" + 0.012*\"专业\" + 0.011*\"教学\" + 0.009*\"学生\" + 0.007*\"综合\" + 0.005*\"地下水\" + 0.005*\"工程\"'),\n",
       " (1,\n",
       "  '0.018*\"消毒\" + 0.018*\"合格率\" + 0.017*\"医疗机构\" + 0.012*\"监测\" + 0.009*\"灭菌\" + 0.007*\"合格\" + 0.006*\"医院\" + 0.006*\"紫外线\" + 0.005*\"样本\" + 0.005*\"2016\"'),\n",
       " (2,\n",
       "  '0.012*\"贫困\" + 0.011*\"模型\" + 0.009*\"农村\" + 0.008*\"算法\" + 0.007*\"耕地\" + 0.006*\"数据\" + 0.006*\"研究\" + 0.005*\"方法\" + 0.005*\"发展\" + 0.005*\"金融\"'),\n",
       " (3,\n",
       "  '0.016*\"创业\" + 0.013*\"发展\" + 0.012*\"创新\" + 0.011*\"大学生\" + 0.010*\"教育\" + 0.009*\"高校\" + 0.009*\"教学\" + 0.009*\"农业\" + 0.008*\"学生\" + 0.008*\"企业\"'),\n",
       " (4,\n",
       "  '0.015*\"执业\" + 0.014*\"医院\" + 0.012*\"项目\" + 0.011*\"多点\" + 0.011*\"医师\" + 0.009*\"医生\" + 0.009*\"影像学\" + 0.008*\"单位\" + 0.008*\"研究\" + 0.007*\"牵头\"'),\n",
       " (5,\n",
       "  '0.024*\"休闲\" + 0.023*\"巢湖\" + 0.021*\"农业\" + 0.013*\"发展\" + 0.011*\"产品\" + 0.011*\"旅游\" + 0.005*\"经营\" + 0.004*\"湿地\" + 0.004*\"建设\" + 0.004*\"文化\"'),\n",
       " (6,\n",
       "  '0.023*\"数据\" + 0.019*\"文本\" + 0.010*\"缺陷\" + 0.010*\"挖掘\" + 0.009*\"电力\" + 0.009*\"技术\" + 0.008*\"分析\" + 0.008*\"数据挖掘\" + 0.006*\"电影\" + 0.006*\"企业\"'),\n",
       " (7,\n",
       "  '0.025*\"医疗\" + 0.023*\"检查\" + 0.020*\"医院\" + 0.016*\"患者\" + 0.013*\"互认\" + 0.012*\"服务\" + 0.008*\"管理\" + 0.007*\"施工\" + 0.006*\"三级\" + 0.006*\"影响\"'),\n",
       " (8,\n",
       "  '0.017*\"教师\" + 0.016*\"教育\" + 0.016*\"成本\" + 0.016*\"施工\" + 0.012*\"道桥\" + 0.011*\"建筑\" + 0.010*\"芬兰\" + 0.010*\"语音\" + 0.007*\"识别\" + 0.007*\"教学\"'),\n",
       " (9,\n",
       "  '0.017*\"新生儿\" + 0.010*\"病房\" + 0.010*\"医院\" + 0.008*\"quinoa\" + 0.008*\"综合\" + 0.007*\"建设\" + 0.005*\"广西\" + 0.005*\"al\" + 0.005*\"Food\" + 0.004*\"能力\"')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "\n",
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "import pandas as pd\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='utf-8',engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) if w not in stoplist and len(w) > 1 ] \n",
    "\n",
    "filelist = [m_cut(w) for w in df.content]\n",
    "\n",
    "\n",
    "# 生成文档对应的字典和bow稀疏向量\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(filelist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in filelist] # 仍为list in list  \n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf  \n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus , id2word = dictionary ,num_topics = 10, passes = 10) \n",
    "ldamodel.print_topics()#打印出10个主题的前十个相关词\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim抽取文档主题并计算文档相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "\n",
    "# 设定分词及清理停用词函数\n",
    "# 熟悉Python的可以使用 open('stopWord.txt').readlines（） 获取停用词list，效率更高\n",
    "import pandas as pd\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='utf-8',engine='python').w)\n",
    "import jieba \n",
    "def m_cut(intxt):\n",
    "    return [ w for w in jieba.cut(intxt) if w not in stoplist and len(w) > 1 ] \n",
    "\n",
    "filelist = [m_cut(w) for w in df.content]\n",
    "\n",
    "\n",
    "# 生成文档对应的字典和bow稀疏向量\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(filelist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in filelist] # 仍为list in list  \n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf  \n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus_tfidf, id2word = dictionary,num_topics = 5, passes = 10) \n",
    "\n",
    "\n",
    "from gensim import similarities\n",
    "simmtx = similarities.MatrixSimilarity(corpus_tfidf) # 使用的矩阵种类需要和拟合模型时相同\n",
    "simmtx\n",
    "print(simmtx.index[:2])\n",
    "\n",
    "# 使用gensim的LDA拟合结果进行演示\n",
    "query = df.content[0] \n",
    "query_bow = dictionary.doc2bow(m_cut(query))\n",
    "query_tfidf = tfidf_model[query_bow]\n",
    "lda_vec = ldamodel[query_tfidf] # 转换为lda模型下的向量\n",
    "sims = simmtx[lda_vec] # 进行矩阵内向量和所提供向量的余弦相似度查询\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
