{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 朴素贝叶斯模型进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'C3-Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'C4-Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'C5-Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'C6-Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'C7-History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'C11-Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'C15-Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'C16-Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'C17-Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'C19-Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'C23-Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'C29-Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'C31-Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'C32-Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'C34-Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'C35-Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'C36-Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'C37-Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'C38-Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'C39-Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "print(xunlian_files.head(9800))\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'C3-Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'C4-Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'C5-Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'C6-Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'C7-History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'C11-Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'C15-Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'C16-Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'C17-Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'C19-Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'C23-Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'C29-Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'C31-Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'C32-Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'C34-Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'C35-Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'C36-Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'C37-Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'C38-Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'C39-Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "xunlian_files.catagory.to_csv(r'C:\\EdmsData\\xunlian_files.txt',sep='\\t', index=False)\n",
    "ceshi_files.catagory.to_csv(r'C:\\EdmsData\\ceshi_files.txt',sep='\\t', index=False)\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "joblib.dump(countvec,'countvec')\n",
    "\n",
    "#----------------------------------拟合朴素贝叶斯模型--------------------------\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#raw12ana.chap 要预测的变量值\n",
    "#x_train, x_test, y_train, y_test = train_test_split(wordmtx_xunlian, xunlian_files.catagory, test_size = 0.3, random_state = 111)\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "NBmodel = naive_bayes.MultinomialNB()\n",
    "\n",
    "# 拟合模型\n",
    "nbf = NBmodel.fit(x_train, y_train)\n",
    "\n",
    "joblib.dump(nbf,'bnf.model')\n",
    "# 进行验证集预测\n",
    "x_test\n",
    "\n",
    "print(NBmodel.predict(x_test))\n",
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', NBmodel.score(x_train, y_train), \n",
    "      '，验证集：', NBmodel.score(x_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, NBmodel.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 使用Logistic回归模型进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.207 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804, 373529)\n",
      "(9804,)\n",
      "<class 'numpy.ndarray'>\n",
      "(9804, 373529)\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Agriculture       0.93      0.94      0.93      1022\n",
      "          Art       0.89      0.95      0.92       713\n",
      "Communication       0.78      0.78      0.78        27\n",
      "     Computer       0.96      0.99      0.97      1358\n",
      "      Economy       0.91      0.94      0.92      1601\n",
      "    Education       0.69      0.54      0.61        61\n",
      "  Electronics       0.83      0.54      0.65        28\n",
      "       Energy       1.00      0.52      0.68        33\n",
      "  Enviornment       0.97      0.97      0.97      1218\n",
      "      History       0.81      0.74      0.78       468\n",
      "          Law       0.71      0.65      0.68        52\n",
      "   Literature       0.64      0.21      0.31        34\n",
      "      Medical       0.92      0.64      0.76        53\n",
      "     Military       0.74      0.68      0.71        76\n",
      "         Mine       0.86      0.71      0.77        34\n",
      "   Philosophy       0.64      0.40      0.49        45\n",
      "     Politics       0.89      0.93      0.91      1026\n",
      "        Space       0.96      0.92      0.94       642\n",
      "       Sports       0.95      0.96      0.96      1254\n",
      "    Transport       0.81      0.73      0.77        59\n",
      "\n",
      "  avg / total       0.92      0.92      0.92      9804\n",
      "\n",
      "['2009_2016年云南省医疗机构消毒灭菌效果趋势评价_李建云_周晓梅', '_一带一路_背景下_国际投资_课_省略_路径探析_以广西民族师范学院为例_郑国富', '三级医院医疗检查结果互认实施现状及影响因素分析_肖晓华_廖惠_梁恒斌_潘振威_苏', '不同等级医院医疗检查结果互认标准研究_肖晓华_谢欣睿_梁恒斌_廖惠_潘振威_苏茹', '乡村振兴战略下环巢湖休闲农业发展研究_沈东生', '二三级医院影像学检查结果互认现状及影响因素研究_肖晓华_苏茹茹_潘振威_廖惠_梁', '关于建筑工程管理的影响因素分析与对策探讨_王君霞_战丙利', '关于彰武县做好互换并地后续工作的建议_张凤华', '关于深化河北省农业水价综合改革的对策建议_马素英', '农村非正规金融发展与农村多维贫困_基于面板门槛模型的研究_吴君娴_黄永兴', '医师多点执业对医院人力资源管理的影响_张妍_李吉', '协同创新优化现代高效农业组织结构_王瑜', '县级公立医院患者体验的影响因素研究_朱锦_胡丹_陈家应', '双创背景下大学生农业创业融资问题探析_王涛_白林_齐骥霆', '吉林省农村金融发展状况及对策研究_孙铁柱', '国家重点研发计划_畜禽_专项立项特征研究_姜玮', '地方高校工科专业课程教学模式改革_省略_临沂大学_大气污染控制工程_为例_马宏卿', '基于大数据的商业智能在电商数据分析中的应用_钱丹丹', '基于数据挖掘技术的档案馆信息快速分析算法研究_甘璐', '基于数据挖掘的电影票房分析_席稼玮', '基于机器视觉和机器学习技术的鸡胴体质量自动分级方法研究_戚超', '工程教育专业认证下自动化专业实践教学体系的构建_黄宜庆_陆华才', '工程造价指数与工程造价动态管理刍议_孙静', '广西综合医院新生儿病房分级建设和能力建设现状调查_胡琴燕_韦琴_黄晓波_杨少丽_', '应用型高校金融学课程教学改革研究_邓旭霞', '建筑工程施工管理及创新技术的应用研究_侯帅', '建筑工程施工管理的进度管理与控制_吕萌', '建筑结构设计中的抗震结构设计理念_马玉洁', '建筑门窗用硅烷改性聚醚胶的性能与应用研究_杨苏邯_陈洋庆_陈建军_龙飞_蒋金博_', '我国医疗检查结果互认制度实施现状_肖晓华_梁恒斌_谢欣睿_苏茹茹_孙刚', '数据挖掘技术在语音识别中的应用_许元洪_郭琼', '数据挖掘技术在风力发电中的应用综述_曾文珺', '智能建筑电气安装工程质量控制要点解析_张凯_张振玉', '河北省高校大学生创业支持体系优化研究_何腾霄_王丽媛_白欣蕊_冯华_李姗姗', '浅析建筑道桥的施工成本把控现状及解决对策_温秀红', '浅析机械设计制造及其自动化中计算机技术的应用_郑宏栗', '浅谈洛阳地区窑居建筑通风采光设计策略_周亚豪_赵余光_梁文涛_陈奕甫_杨梦辉', '浙江医疗卫生服务领域_最多跑一次_改革政策分析_胡重明', '电力文本数据挖掘现状及挑战_王慧芳', '绥化市农业电子商务发展模式研究_余娟_赵艳_孙晓_李冰_张云晖', '耕地资源富集区县域贫困格局及其影响机制_以黑龙江省兰西县为例_杜国明', '芬兰教育的核心竞争力_高质量的教师队伍_刘蕊', '药品零加成背景下县级公立医院绩效改革困境_刘丽杭_陶飞旸', '藜麦营养功能与开发利用进展_王启明', '解析装配式建筑工程施工过程中BIM技术的应用_刘志文', '辽宁省特色人才培养模式研究_郭晓林_徐靓', '遗传算法的数据挖掘技术在医疗大数据中的应用研究_陈萌', '面向调度运行分析的电网数据分析与挖掘_张英华', '高校大学生就业指导中的思想政治教育研究_张梦君', '高校油画专业课堂教学改革探究_舒文鑫']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Enviornment', 'Economy', 'Medical', 'Medical', 'Agriculture',\n",
       "       'Medical', 'Sports', 'Agriculture', 'Agriculture', 'Agriculture',\n",
       "       'Medical', 'Economy', 'Medical', 'Agriculture', 'Agriculture',\n",
       "       'Agriculture', 'Sports', 'Economy', 'Computer', 'Sports',\n",
       "       'Computer', 'Sports', 'Law', 'Medical', 'Economy', 'Enviornment',\n",
       "       'Transport', 'Space', 'Enviornment', 'Medical', 'Computer',\n",
       "       'Computer', 'Transport', 'Sports', 'Transport', 'Computer',\n",
       "       'Energy', 'Medical', 'Energy', 'Economy', 'Agriculture', 'Sports',\n",
       "       'Medical', 'Agriculture', 'Sports', 'Education', 'Medical', 'Space',\n",
       "       'Education', 'Sports'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(countvec,'countvec_logitmodel')\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logitmodel = LogisticRegression() # 定义Logistic回归模型\n",
    "# 拟合模型\n",
    "lgf = logitmodel.fit(x_train, y_train)\n",
    "print(classification_report(y_test, logitmodel.predict(x_test)))\n",
    "#保存模型\n",
    "joblib.dump(lgf,'logitmodel.model')\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "\n",
    "x_test = countvec.transform(rawfile)\n",
    "\n",
    "logitmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 导入logitmodel.model，进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2009_2016年云南省医疗机构消毒灭菌效果趋势评价_李建云_周晓梅', '_一带一路_背景下_国际投资_课_省略_路径探析_以广西民族师范学院为例_郑国富', '三级医院医疗检查结果互认实施现状及影响因素分析_肖晓华_廖惠_梁恒斌_潘振威_苏', '不同等级医院医疗检查结果互认标准研究_肖晓华_谢欣睿_梁恒斌_廖惠_潘振威_苏茹', '乡村振兴战略下环巢湖休闲农业发展研究_沈东生', '二三级医院影像学检查结果互认现状及影响因素研究_肖晓华_苏茹茹_潘振威_廖惠_梁', '关于建筑工程管理的影响因素分析与对策探讨_王君霞_战丙利', '关于彰武县做好互换并地后续工作的建议_张凤华', '关于深化河北省农业水价综合改革的对策建议_马素英', '农村非正规金融发展与农村多维贫困_基于面板门槛模型的研究_吴君娴_黄永兴', '医师多点执业对医院人力资源管理的影响_张妍_李吉', '协同创新优化现代高效农业组织结构_王瑜', '县级公立医院患者体验的影响因素研究_朱锦_胡丹_陈家应', '双创背景下大学生农业创业融资问题探析_王涛_白林_齐骥霆', '吉林省农村金融发展状况及对策研究_孙铁柱', '国家重点研发计划_畜禽_专项立项特征研究_姜玮', '地方高校工科专业课程教学模式改革_省略_临沂大学_大气污染控制工程_为例_马宏卿', '基于大数据的商业智能在电商数据分析中的应用_钱丹丹', '基于数据挖掘技术的档案馆信息快速分析算法研究_甘璐', '基于数据挖掘的电影票房分析_席稼玮', '基于机器视觉和机器学习技术的鸡胴体质量自动分级方法研究_戚超', '工程教育专业认证下自动化专业实践教学体系的构建_黄宜庆_陆华才', '工程造价指数与工程造价动态管理刍议_孙静', '广西综合医院新生儿病房分级建设和能力建设现状调查_胡琴燕_韦琴_黄晓波_杨少丽_', '应用型高校金融学课程教学改革研究_邓旭霞', '建筑工程施工管理及创新技术的应用研究_侯帅', '建筑工程施工管理的进度管理与控制_吕萌', '建筑结构设计中的抗震结构设计理念_马玉洁', '建筑门窗用硅烷改性聚醚胶的性能与应用研究_杨苏邯_陈洋庆_陈建军_龙飞_蒋金博_', '我国医疗检查结果互认制度实施现状_肖晓华_梁恒斌_谢欣睿_苏茹茹_孙刚', '数据挖掘技术在语音识别中的应用_许元洪_郭琼', '数据挖掘技术在风力发电中的应用综述_曾文珺', '智能建筑电气安装工程质量控制要点解析_张凯_张振玉', '河北省高校大学生创业支持体系优化研究_何腾霄_王丽媛_白欣蕊_冯华_李姗姗', '浅析建筑道桥的施工成本把控现状及解决对策_温秀红', '浅析机械设计制造及其自动化中计算机技术的应用_郑宏栗', '浅谈洛阳地区窑居建筑通风采光设计策略_周亚豪_赵余光_梁文涛_陈奕甫_杨梦辉', '浙江医疗卫生服务领域_最多跑一次_改革政策分析_胡重明', '电力文本数据挖掘现状及挑战_王慧芳', '绥化市农业电子商务发展模式研究_余娟_赵艳_孙晓_李冰_张云晖', '耕地资源富集区县域贫困格局及其影响机制_以黑龙江省兰西县为例_杜国明', '芬兰教育的核心竞争力_高质量的教师队伍_刘蕊', '药品零加成背景下县级公立医院绩效改革困境_刘丽杭_陶飞旸', '藜麦营养功能与开发利用进展_王启明', '解析装配式建筑工程施工过程中BIM技术的应用_刘志文', '辽宁省特色人才培养模式研究_郭晓林_徐靓', '遗传算法的数据挖掘技术在医疗大数据中的应用研究_陈萌', '面向调度运行分析的电网数据分析与挖掘_张英华', '高校大学生就业指导中的思想政治教育研究_张梦君', '高校油画专业课堂教学改革探究_舒文鑫']\n",
      "['Enviornment' 'Economy' 'Medical' 'Medical' 'Agriculture' 'Medical'\n",
      " 'Sports' 'Agriculture' 'Agriculture' 'Agriculture' 'Medical' 'Economy'\n",
      " 'Medical' 'Agriculture' 'Agriculture' 'Agriculture' 'Sports' 'Economy'\n",
      " 'Computer' 'Sports' 'Computer' 'Sports' 'Law' 'Medical' 'Economy'\n",
      " 'Enviornment' 'Transport' 'Space' 'Enviornment' 'Medical' 'Computer'\n",
      " 'Computer' 'Transport' 'Sports' 'Transport' 'Computer' 'Energy' 'Medical'\n",
      " 'Energy' 'Economy' 'Agriculture' 'Sports' 'Medical' 'Agriculture' 'Sports'\n",
      " 'Education' 'Medical' 'Space' 'Education' 'Sports']\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "nbf =joblib.load('logitmodel.model')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "file1 = [' '.join(df2.fenci[0])]\n",
    "\n",
    "\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "\n",
    "countvec=joblib.load('countvec_logitmodel')\n",
    "x_test = countvec.transform(rawfile)\n",
    "\n",
    "catagory_list = nbf.predict(x_test)\n",
    "print(catagory_list)\n",
    "print(type(catagory_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 随机森林文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804, 373529)\n",
      "(9804,)\n",
      "<class 'numpy.ndarray'>\n",
      "(9804, 373529)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "精度:0.803\n",
      "召回:0.805\n",
      "f1-score:0.788\n",
      "训练集： 0.997246022032 ，验证集： 0.805283557732\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Agriculture       0.82      0.85      0.83      1022\n",
      "          Art       0.74      0.90      0.81       713\n",
      "Communication       0.43      0.44      0.44        27\n",
      "     Computer       0.88      0.98      0.92      1358\n",
      "      Economy       0.74      0.89      0.81      1601\n",
      "    Education       0.67      0.20      0.30        61\n",
      "  Electronics       1.00      0.04      0.07        28\n",
      "       Energy       0.80      0.12      0.21        33\n",
      "  Enviornment       0.93      0.91      0.92      1218\n",
      "      History       0.61      0.21      0.31       468\n",
      "          Law       0.62      0.15      0.25        52\n",
      "   Literature       0.67      0.06      0.11        34\n",
      "      Medical       0.70      0.36      0.48        53\n",
      "     Military       0.69      0.14      0.24        76\n",
      "         Mine       0.60      0.18      0.27        34\n",
      "   Philosophy       0.62      0.22      0.33        45\n",
      "     Politics       0.67      0.79      0.72      1026\n",
      "        Space       0.95      0.73      0.83       642\n",
      "       Sports       0.86      0.85      0.86      1254\n",
      "    Transport       0.77      0.17      0.28        59\n",
      "\n",
      "  avg / total       0.80      0.81      0.79      9804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#拟合模型\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(oob_score=True, random_state=20)   \n",
    "rfcf = clf.fit(x_train, y_train)\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rfcf,'RandomForestClassifier_model')\n",
    "joblib.dump(countvec,'RandomForestClassifier_countvec')\n",
    "predicted = clf.predict(x_test) \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "predicted\n",
    "\n",
    "# 计算分类精度：  \n",
    "from sklearn import metrics  \n",
    "def metrics_result(actual, predict):  \n",
    "    print('精度:%.3f' %metrics.precision_score(actual, predict,average='weighted'))\n",
    "    print('召回:%.3f'%metrics.recall_score(actual, predict,average='weighted'))\n",
    "    print('f1-score:%.3f'%metrics.f1_score(actual, predict,average='weighted'))\n",
    "metrics_result(y_test, predicted)\n",
    "\n",
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', rfcf.score(x_train, y_train), \n",
    "      '，验证集：', rfcf.score(x_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, rfcf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.067 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'countvec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-04baedde7170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#rawfile listoflist ,每一项为用空格连接的一篇文档\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcountvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfcf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countvec' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "rfcf = joblib.load('RandomForestClassifier_model')\n",
    "countvec = joblib.load('RandomForestClassifier_countvec')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "file1 = [' '.join(df2.fenci[0])]\n",
    "\n",
    "\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "\n",
    "x_test = countvec.transform(rawfile)\n",
    "predicted = rfcf.predict(x_test) \n",
    "print(predicted)\n",
    "for i in range(len(predicted)):\n",
    "    print(df.filename[i]+\":\"+predicted[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 决策树分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.261 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804, 373529)\n",
      "(9804,)\n",
      "<class 'numpy.ndarray'>\n",
      "(9804, 373529)\n",
      "Training score:0.999796\n",
      "Testing score:0.888413\n",
      "训练集： 0.999796001632 ，验证集： 0.888412892697\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Agriculture       0.93      0.94      0.93      1022\n",
      "          Art       0.81      0.92      0.86       713\n",
      "Communication       0.45      0.56      0.50        27\n",
      "     Computer       0.97      0.97      0.97      1358\n",
      "      Economy       0.90      0.93      0.91      1601\n",
      "    Education       0.61      0.44      0.51        61\n",
      "  Electronics       0.67      0.43      0.52        28\n",
      "       Energy       0.35      0.45      0.39        33\n",
      "  Enviornment       0.96      0.95      0.95      1218\n",
      "      History       0.72      0.61      0.66       468\n",
      "          Law       0.51      0.42      0.46        52\n",
      "   Literature       0.36      0.24      0.29        34\n",
      "      Medical       0.60      0.45      0.52        53\n",
      "     Military       0.44      0.28      0.34        76\n",
      "         Mine       0.58      0.44      0.50        34\n",
      "   Philosophy       0.49      0.44      0.47        45\n",
      "     Politics       0.82      0.84      0.83      1026\n",
      "        Space       0.93      0.95      0.94       642\n",
      "       Sports       0.95      0.93      0.94      1254\n",
      "    Transport       0.55      0.56      0.55        59\n",
      "\n",
      "  avg / total       0.89      0.89      0.89      9804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#拟合模型\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier() \n",
    "rfcf = clf.fit(x_train, y_train)\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(rfcf,'DecisionTreeClassifier_model')\n",
    "joblib.dump(countvec,'DecisionTreeClassifierr_countvec')\n",
    "predicted = clf.predict(x_test) \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "predicted\n",
    "\n",
    "print(\"Training score:%f\" % (clf.score(x_train, y_train)))\n",
    "print(\"Testing score:%f\" % (clf.score(x_test, y_test)))\n",
    "\n",
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', rfcf.score(x_train, y_train), \n",
    "      '，验证集：', rfcf.score(x_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, rfcf.predict(x_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.414 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804, 373529)\n",
      "(9804,)\n",
      "<class 'numpy.ndarray'>\n",
      "(9804, 373529)\n",
      "['Economy' 'Art' 'Art' ..., 'Sports' 'Sports' 'Sports']\n",
      "训练集： 0.99949000408 ，验证集： 0.923806609547\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Agriculture       0.97      0.96      0.97      1022\n",
      "          Art       0.90      0.94      0.92       713\n",
      "Communication       0.90      0.33      0.49        27\n",
      "     Computer       0.99      0.99      0.99      1358\n",
      "      Economy       0.90      0.95      0.92      1601\n",
      "    Education       0.73      0.13      0.22        61\n",
      "  Electronics       0.67      0.21      0.32        28\n",
      "       Energy       0.88      0.21      0.34        33\n",
      "  Enviornment       0.98      0.98      0.98      1218\n",
      "      History       0.87      0.81      0.84       468\n",
      "          Law       0.67      0.23      0.34        52\n",
      "   Literature       0.73      0.24      0.36        34\n",
      "      Medical       0.86      0.36      0.51        53\n",
      "     Military       0.86      0.24      0.37        76\n",
      "         Mine       0.76      0.38      0.51        34\n",
      "   Philosophy       0.86      0.40      0.55        45\n",
      "     Politics       0.77      0.95      0.85      1026\n",
      "        Space       0.99      0.97      0.98       642\n",
      "       Sports       0.97      0.97      0.97      1254\n",
      "    Transport       0.86      0.41      0.55        59\n",
      "\n",
      "  avg / total       0.92      0.92      0.92      9804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#拟合模型\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=20)\n",
    "gbcf = gbc.fit(x_train, y_train)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gbcf,'GBDT_model')\n",
    "joblib.dump(countvec,'GBDT_countvec')\n",
    "predicted = gbc.predict(x_test) \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "print(predicted)\n",
    "#print(NBmodel.predict(x_test))\n",
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', gbc.score(x_train, y_train), \n",
    "      '，验证集：', gbc.score(x_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, gbc.predict(x_test)))\n",
    "\n",
    "#print(\"Training score:%f\" % (clf.score(x_train, y_train)))\n",
    "#print(\"Testing score:%f\" % (clf.score(x_test, y_test)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.244 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9804, 373529)\n",
      "(9804,)\n",
      "<class 'numpy.ndarray'>\n",
      "(9804, 373529)\n",
      "['Economy' 'History' 'Economy' ..., 'Computer' 'Computer' 'Computer']\n",
      "训练集： 0.416360669115 ，验证集： 0.417890656875\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "  Agriculture       0.17      0.02      0.04      1022\n",
      "          Art       0.13      0.02      0.03       713\n",
      "Communication       0.30      0.74      0.43        27\n",
      "     Computer       0.43      0.95      0.59      1358\n",
      "      Economy       0.39      0.81      0.53      1601\n",
      "    Education       0.00      0.00      0.00        61\n",
      "  Electronics       0.00      0.00      0.00        28\n",
      "       Energy       0.51      0.55      0.53        33\n",
      "  Enviornment       0.55      0.29      0.38      1218\n",
      "      History       0.18      0.20      0.19       468\n",
      "          Law       0.00      0.00      0.00        52\n",
      "   Literature       0.00      0.00      0.00        34\n",
      "      Medical       0.12      0.04      0.06        53\n",
      "     Military       0.00      0.00      0.00        76\n",
      "         Mine       0.80      0.47      0.59        34\n",
      "   Philosophy       0.83      0.53      0.65        45\n",
      "     Politics       0.25      0.30      0.27      1026\n",
      "        Space       0.14      0.00      0.00       642\n",
      "       Sports       0.92      0.51      0.66      1254\n",
      "    Transport       0.00      0.00      0.00        59\n",
      "\n",
      "  avg / total       0.39      0.42      0.35      9804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#1.生成d2m矩阵\n",
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data\\train_corpus\\C4-Literature'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Literature')\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data\\train_corpus\\C5-Education'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Education')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data\\train_corpus\\C6-Philosophy'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Philosophy')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data\\train_corpus\\C7-History'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'History')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data\\train_corpus\\C11-Space'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Space')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data\\train_corpus\\C15-Energy'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Energy')\n",
    "\n",
    "rootdir8 = r'C:\\EdmsData\\data\\train_corpus\\C16-Electronics'\n",
    "xunlian_files8 = new_DataFrame(rootdir8,'Electronics')\n",
    "\n",
    "rootdir9 = r'C:\\EdmsData\\data\\train_corpus\\C17-Communication'\n",
    "xunlian_files9 = new_DataFrame(rootdir9,'Communication')\n",
    "\n",
    "rootdir10 = r'C:\\EdmsData\\data\\train_corpus\\C19-Computer'\n",
    "xunlian_files10 = new_DataFrame(rootdir10,'Computer')\n",
    "\n",
    "rootdir11 = r'C:\\EdmsData\\data\\train_corpus\\C23-Mine'\n",
    "xunlian_files11 = new_DataFrame(rootdir11,'Mine')\n",
    "\n",
    "rootdir12 = r'C:\\EdmsData\\data\\train_corpus\\C29-Transport'\n",
    "xunlian_files12 = new_DataFrame(rootdir12,'Transport')\n",
    "\n",
    "rootdir13 = r'C:\\EdmsData\\data\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files13 = new_DataFrame(rootdir13,'Enviornment')\n",
    "\n",
    "rootdir14 = r'C:\\EdmsData\\data\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files14 = new_DataFrame(rootdir14,'Agriculture')\n",
    "\n",
    "rootdir15 = r'C:\\EdmsData\\data\\train_corpus\\C34-Economy'\n",
    "xunlian_files15 = new_DataFrame(rootdir15,'Economy')\n",
    "\n",
    "rootdir16 = r'C:\\EdmsData\\data\\train_corpus\\C35-Law'\n",
    "xunlian_files16 = new_DataFrame(rootdir16,'Law')\n",
    "\n",
    "rootdir17 = r'C:\\EdmsData\\data\\train_corpus\\C36-Medical'\n",
    "xunlian_files17 = new_DataFrame(rootdir17,'Medical')\n",
    "\n",
    "rootdir18 = r'C:\\EdmsData\\data\\train_corpus\\C37-Military'\n",
    "xunlian_files18 = new_DataFrame(rootdir18,'Military')\n",
    "\n",
    "rootdir19 = r'C:\\EdmsData\\data\\train_corpus\\C38-Politics'\n",
    "xunlian_files19 = new_DataFrame(rootdir19,'Politics')\n",
    "\n",
    "rootdir20 = r'C:\\EdmsData\\data\\train_corpus\\C39-Sports'\n",
    "xunlian_files20 = new_DataFrame(rootdir20,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files8,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files9,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files10,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files11,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files12,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files13,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files14,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files15,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files16,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files17,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files18,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files19,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files20,ignore_index=True)\n",
    "\n",
    "xunlian_files #训练集\n",
    "\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "rootdir21 = r'C:\\EdmsData\\data\\test_corpus\\C3-Art'\n",
    "ceshi_files = new_DataFrame(rootdir21,'Art')\n",
    "\n",
    "rootdir22 = r'C:\\EdmsData\\data\\test_corpus\\C4-Literature'\n",
    "ceshi_files2 = new_DataFrame(rootdir22,'Literature')\n",
    "\n",
    "rootdir23 = r'C:\\EdmsData\\data\\test_corpus\\C5-Education'\n",
    "ceshi_files3 = new_DataFrame(rootdir23,'Education')\n",
    "\n",
    "rootdir24 = r'C:\\EdmsData\\data\\test_corpus\\C6-Philosophy'\n",
    "ceshi_files4 = new_DataFrame(rootdir24,'Philosophy')\n",
    "\n",
    "rootdir25 = r'C:\\EdmsData\\data\\test_corpus\\C7-History'\n",
    "ceshi_files5 = new_DataFrame(rootdir25,'History')\n",
    "\n",
    "rootdir26 = r'C:\\EdmsData\\data\\test_corpus\\C11-Space'\n",
    "ceshi_files6 = new_DataFrame(rootdir26,'Space')\n",
    "\n",
    "rootdir27 = r'C:\\EdmsData\\data\\test_corpus\\C15-Energy'\n",
    "ceshi_files7 = new_DataFrame(rootdir27,'Energy')\n",
    "\n",
    "rootdir28 = r'C:\\EdmsData\\data\\test_corpus\\C16-Electronics'\n",
    "ceshi_files8 = new_DataFrame(rootdir28,'Electronics')\n",
    "\n",
    "rootdir29 = r'C:\\EdmsData\\data\\test_corpus\\C17-Communication'\n",
    "ceshi_files9 = new_DataFrame(rootdir29,'Communication')\n",
    "\n",
    "rootdir30 = r'C:\\EdmsData\\data\\test_corpus\\C19-Computer'\n",
    "ceshi_files10 = new_DataFrame(rootdir30,'Computer')\n",
    "\n",
    "rootdir31 = r'C:\\EdmsData\\data\\test_corpus\\C23-Mine'\n",
    "ceshi_files11 = new_DataFrame(rootdir31,'Mine')\n",
    "\n",
    "rootdir32 = r'C:\\EdmsData\\data\\test_corpus\\C29-Transport'\n",
    "ceshi_files12 = new_DataFrame(rootdir32,'Transport')\n",
    "\n",
    "rootdir33 = r'C:\\EdmsData\\data\\test_corpus\\C31-Enviornment'\n",
    "ceshi_files13 = new_DataFrame(rootdir33,'Enviornment')\n",
    "\n",
    "rootdir34 = r'C:\\EdmsData\\data\\test_corpus\\C32-Agriculture'\n",
    "ceshi_files14 = new_DataFrame(rootdir34,'Agriculture')\n",
    "\n",
    "rootdir35 = r'C:\\EdmsData\\data\\test_corpus\\C34-Economy'\n",
    "ceshi_files15 = new_DataFrame(rootdir35,'Economy')\n",
    "\n",
    "rootdir36 = r'C:\\EdmsData\\data\\test_corpus\\C35-Law'\n",
    "ceshi_files16 = new_DataFrame(rootdir36,'Law')\n",
    "\n",
    "rootdir37 = r'C:\\EdmsData\\data\\test_corpus\\C36-Medical'\n",
    "ceshi_files17 = new_DataFrame(rootdir37,'Medical')\n",
    "\n",
    "rootdir38 = r'C:\\EdmsData\\data\\test_corpus\\C37-Military'\n",
    "ceshi_files18 = new_DataFrame(rootdir38,'Military')\n",
    "\n",
    "rootdir39 = r'C:\\EdmsData\\data\\test_corpus\\C38-Politics'\n",
    "ceshi_files19 = new_DataFrame(rootdir39,'Politics')\n",
    "\n",
    "rootdir40 = r'C:\\EdmsData\\data\\test_corpus\\C39-Sports'\n",
    "ceshi_files20 = new_DataFrame(rootdir40,'Sports')\n",
    "\n",
    "ceshi_files = ceshi_files.append(ceshi_files2,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files3,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files4,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files5,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files6,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files7,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files8,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files9,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files10,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files11,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files12,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files13,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files14,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files15,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files16,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files17,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files18,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files19,ignore_index=True)\n",
    "ceshi_files = ceshi_files.append(ceshi_files20,ignore_index=True)\n",
    "\n",
    "ceshi_files\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "ceshi_files['cleantxt'] = ceshi_files.content.apply(cuttxt)\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "\n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "ceshi_files_catagory = []\n",
    "for i in range(len(ceshi_files)):\n",
    "    ceshi_files_catagory.append(ceshi_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(xunlian_files_catagory)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "x_test = countvec.transform(ceshi_files.cleantxt)\n",
    "print(x_test.shape)\n",
    "y_test = np.array(ceshi_files_catagory)\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "#拟合模型\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "abc = AdaBoostClassifier()\n",
    "abcf = abc.fit(x_train, y_train)\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(abcf,'ABC_model')\n",
    "joblib.dump(countvec,'ABC_countvec')\n",
    "predicted = abc.predict(x_test) \n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "print(predicted)\n",
    "#print(NBmodel.predict(x_test))\n",
    "# 预测准确率（给模型打分）\n",
    "print('训练集：', abcf.score(x_train, y_train), \n",
    "      '，验证集：', abcf.score(x_test, y_test))\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, abcf.predict(x_test)))\n",
    "\n",
    "#print(\"Training score:%f\" % (clf.score(x_train, y_train)))\n",
    "#print(\"Testing score:%f\" % (clf.score(x_test, y_test)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
