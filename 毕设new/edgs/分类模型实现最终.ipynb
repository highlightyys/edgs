{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7类-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.066 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data2\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'C3-Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data2\\train_corpus\\C19-Computer'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'C19-Computer')\n",
    "\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data2\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'C31-Enviornment')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data2\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'C32-Agriculture')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data2\\train_corpus\\C34-Economy'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'C34-Economy')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data2\\train_corpus\\C38-Politics'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'C38-Politics')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data2\\train_corpus\\C39-Sports'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'C39-Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "\n",
    "#xunlian_files #训练集\n",
    "#print(xunlian_files.head(9800))\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='UTF-8',engine='python').w)\n",
    "cuttxt = lambda x : ' '.join([w for w in jieba.cut(x) if w not in stoplist and len(w) >1])\n",
    "\n",
    "#cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train_wordmtx = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()  \n",
    "x_train_tfidf = transformer.fit_transform(x_train_wordmtx)\n",
    "\n",
    "#print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train0 = np.array(xunlian_files_catagory)\n",
    "\n",
    "\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#raw12ana.chap 要预测的变量值\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfidf, y_train0, test_size = 0.3, random_state = 7)\n",
    "\n",
    "\n",
    "#----------------------------------拟合模型--------------------------\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#设置评估算法的基准\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "#生成算法模型\n",
    "models = {}\n",
    "models['LR'] = LogisticRegression()\n",
    "models['SVM'] = SVC()\n",
    "models['CART'] =DecisionTreeClassifier()\n",
    "models['MNB'] = MultinomialNB()\n",
    "models['KNN'] = KNeighborsClassifier()\n",
    "models['RF'] = RandomForestClassifier()\n",
    "models['AB'] = AdaBoostClassifier()\n",
    "from sklearn.metrics import classification_report\n",
    "results = []\n",
    "for key in models:  \n",
    "    kfold = KFold(n_splits = num_folds,random_state = seed)\n",
    "    cv_results = cross_val_score(models[key],x_train_tfidf,y_train0,cv = kfold,scoring = scoring)\n",
    "    results.append(cv_results)\n",
    "    print('%s : %f (%f)' %(key,cv_results.mean(),cv_results.std()))\n",
    "    \n",
    "#箱线图\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparision')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(models.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优 ： 0.975510204082 使用 {'C': 18}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data2\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'C3-Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data2\\train_corpus\\C19-Computer'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'C19-Computer')\n",
    "\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data2\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'C31-Enviornment')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data2\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'C32-Agriculture')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data2\\train_corpus\\C34-Economy'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'C34-Economy')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data2\\train_corpus\\C38-Politics'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'C38-Politics')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data2\\train_corpus\\C39-Sports'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'C39-Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "\n",
    "#xunlian_files #训练集\n",
    "#print(xunlian_files.head(9800))\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='UTF-8',engine='python').w)\n",
    "cuttxt = lambda x : ' '.join([w for w in jieba.cut(x) if w not in stoplist and len(w) >1])\n",
    "\n",
    "#cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train_wordmtx = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()  \n",
    "x_train_tfidf = transformer.fit_transform(x_train_wordmtx)\n",
    "\n",
    "#print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train0 = np.array(xunlian_files_catagory)\n",
    "\n",
    "\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#raw12ana.chap 要预测的变量值\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfidf, y_train0, test_size = 0.3, random_state = 7)\n",
    "\n",
    "\n",
    "#----------------------------------拟合模型--------------------------\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#设置评估算法的基准\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "#生成算法模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "param_grid['C'] = [0.1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits = num_folds,random_state = seed)\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grid,scoring = scoring,cv=kfold)\n",
    "grid_result = grid.fit(X=x_train,y = y_train)\n",
    "print('最优 ： %s 使用 %s' % (grid_result.best_score_,grid_result.best_params_))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.423 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优 ： 0.975510204082 使用 {'C': 18}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data2\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'C3-Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data2\\train_corpus\\C19-Computer'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'C19-Computer')\n",
    "\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data2\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'C31-Enviornment')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data2\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'C32-Agriculture')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data2\\train_corpus\\C34-Economy'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'C34-Economy')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data2\\train_corpus\\C38-Politics'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'C38-Politics')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data2\\train_corpus\\C39-Sports'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'C39-Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "\n",
    "#xunlian_files #训练集\n",
    "#print(xunlian_files.head(9800))\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='UTF-8',engine='python').w)\n",
    "cuttxt = lambda x : ' '.join([w for w in jieba.cut(x) if w not in stoplist and len(w) >1])\n",
    "\n",
    "#cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train_wordmtx = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()  \n",
    "x_train_tfidf = transformer.fit_transform(x_train_wordmtx)\n",
    "\n",
    "#print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train0 = np.array(xunlian_files_catagory)\n",
    "\n",
    "\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#raw12ana.chap 要预测的变量值\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfidf, y_train0, test_size = 0.3, random_state = 7)\n",
    "\n",
    "\n",
    "#----------------------------------拟合模型--------------------------\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#设置评估算法的基准\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "#生成算法模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {}\n",
    "param_grid['C'] = [0.1,1,5,13,15,18,19]\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits = num_folds,random_state = seed)\n",
    "grid = GridSearchCV(estimator = model,param_grid =\n",
    "                param_grid,scoring = scoring,cv=kfold)\n",
    "grid_result = grid.fit(X=x_train,y = y_train)\n",
    "print('最优 ： %s 使用 %s' % (grid_result.best_score_,\n",
    "                         grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "Agriculture       0.97      0.97      0.97       466\n",
      "        Art       0.99      1.00      0.99       444\n",
      "   Computer       0.97      0.99      0.98       451\n",
      "    Economy       0.94      0.96      0.95       427\n",
      "Enviornment       0.98      0.97      0.98       468\n",
      "   Politics       0.98      0.94      0.96       467\n",
      "     Sports       0.97      0.97      0.97       427\n",
      "\n",
      "avg / total       0.97      0.97      0.97      3150\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LR_model']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "#读入训练集的文件，形成矩阵\n",
    "def list_all_files(rootdir):\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(PdfToTxt.list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(list[i])\n",
    "    return _files\n",
    "\n",
    "def gettxt_list(rootdir):\n",
    "    all_list = list_all_files(rootdir)   \n",
    "    txt_list=[]\n",
    "    for onelist in all_list:\n",
    "        if onelist.find('txt') != -1:\n",
    "            txt_list.append(onelist[0:-4])   \n",
    "        #print(txt_list)\n",
    "    return txt_list\n",
    "\n",
    "def readtxt(filename,rootdir):\n",
    "    #读取一个txt文件，内容保存在str中\n",
    "    f=open(rootdir+'/'+filename+'.txt','r',encoding='ANSI',errors='ignore')\n",
    "    str = ''\n",
    "    for line in f.readlines():\n",
    "        a = line.strip('\\n')\n",
    "        str = str + a\n",
    "        #print(str)\n",
    "    return str\n",
    "\n",
    "def getAlldata(rootdir):\n",
    "    filesdata = []\n",
    "    txt_lists = gettxt_list(rootdir)\n",
    "        #print(txt_lists)\n",
    "    for i in range(len(txt_lists)):\n",
    "        filename = txt_lists[i]\n",
    "        file_content = readtxt(filename,rootdir)\n",
    "        listi = [filename,file_content]\n",
    "        filesdata.append(listi)\n",
    "        #print(data)\n",
    "    return filesdata\n",
    "\n",
    "def new_DataFrame(rootdir,catagory): \n",
    "    filesdata= getAlldata(rootdir)\n",
    "    df_files = pd.DataFrame(data=filesdata,columns = ['filename','content'])\n",
    "    df_files['catagory'] = catagory\n",
    "    return df_files\n",
    "\n",
    "rootdir1 = r'C:\\EdmsData\\data2\\train_corpus\\C3-Art'\n",
    "xunlian_files = new_DataFrame(rootdir1,'Art')\n",
    "\n",
    "rootdir2 = r'C:\\EdmsData\\data2\\train_corpus\\C19-Computer'\n",
    "xunlian_files2 = new_DataFrame(rootdir2,'Computer')\n",
    "\n",
    "\n",
    "rootdir3 = r'C:\\EdmsData\\data2\\train_corpus\\C31-Enviornment'\n",
    "xunlian_files3 = new_DataFrame(rootdir3,'Enviornment')\n",
    "\n",
    "rootdir4 = r'C:\\EdmsData\\data2\\train_corpus\\C32-Agriculture'\n",
    "xunlian_files4 = new_DataFrame(rootdir4,'Agriculture')\n",
    "\n",
    "rootdir5 = r'C:\\EdmsData\\data2\\train_corpus\\C34-Economy'\n",
    "xunlian_files5 = new_DataFrame(rootdir5,'Economy')\n",
    "\n",
    "rootdir6 = r'C:\\EdmsData\\data2\\train_corpus\\C38-Politics'\n",
    "xunlian_files6 = new_DataFrame(rootdir6,'Politics')\n",
    "\n",
    "rootdir7 = r'C:\\EdmsData\\data2\\train_corpus\\C39-Sports'\n",
    "xunlian_files7 = new_DataFrame(rootdir7,'Sports')\n",
    "\n",
    "xunlian_files = xunlian_files.append(xunlian_files2,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files3,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files4,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files5,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files6,ignore_index=True)\n",
    "xunlian_files = xunlian_files.append(xunlian_files7,ignore_index=True)\n",
    "\n",
    "#xunlian_files #训练集\n",
    "#print(xunlian_files.head(9800))\n",
    "\n",
    "#----------------------------------形成训练集的数据框---------------------------\n",
    "\n",
    "#----------------------------------形成测试集的数据框---------------------------\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "#分词\n",
    "import jieba\n",
    "\n",
    "stoplist = list(pd.read_csv('C:/TMdata/stopwords.txt',names=['w'],sep='aaa',encoding='UTF-8',engine='python').w)\n",
    "cuttxt = lambda x : ' '.join([w for w in jieba.cut(x) if w not in stoplist and len(w) >1])\n",
    "\n",
    "#cuttxt = lambda x : ' '.join(jieba.lcut(x))\n",
    "xunlian_files['cleantxt'] = xunlian_files.content.apply(cuttxt)\n",
    "\n",
    "\n",
    "#词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvec = CountVectorizer() \n",
    "xunlian_files_catagory = []\n",
    "for i in range(len(xunlian_files)):\n",
    "    xunlian_files_catagory.append(xunlian_files.catagory[i])\n",
    "\n",
    "\n",
    "x_train_wordmtx = countvec.fit_transform(xunlian_files.cleantxt)\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()  \n",
    "x_train_tfidf = transformer.fit_transform(x_train_wordmtx)\n",
    "\n",
    "#print(x_train.shape)\n",
    "\n",
    "import numpy as np\n",
    "y_train0 = np.array(xunlian_files_catagory)\n",
    "\n",
    "\n",
    "# 作用：将数据集划分为 训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "#raw12ana.chap 要预测的变量值\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train_tfidf, y_train0, test_size = 0.3, random_state = 7)\n",
    "\n",
    "\n",
    "#----------------------------------拟合模型--------------------------\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#设置评估算法的基准\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "#生成算法模型\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "lmodel = LogisticRegression(random_state = seed,C=18)\n",
    "LR_model = lmodel.fit(x_train,y_train)\n",
    "print(classification_report(y_test, lmodel.predict(x_test)))\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(countvec,'countvec_LR')\n",
    "joblib.dump(transformer,'transformer_LR')\n",
    "joblib.dump(LR_model,'LR_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
