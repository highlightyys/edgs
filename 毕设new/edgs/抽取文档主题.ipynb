{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.主题模型的sklearn实现\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#先弄出文档词频矩阵，然后进行相应的模型训练\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "countvec = CountVectorizer(min_df = 5)#最小词频\n",
    "X = countvec.fit_transform(rawfile)#稀疏矩阵\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "#基于词频矩阵X计算TF-IDF值\n",
    "#---------------------------------\n",
    "#设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics=3#提取主题个数\n",
    "ldamodel = LatentDirichletAllocation(n_components= n_topics)\n",
    "#拟合LDA模型\n",
    "ldamodel.fit(tfidf)\n",
    "#拟合后的模型实质\n",
    "print(ldamodel.components_.shape)#(10,705)\n",
    "ldamodel.components_[:2]#前两条，每个主题中每个词条出现的概率\n",
    "\n",
    "#打印主题词\n",
    "def print_top_words(model,feature_names,n_top_words):\n",
    "    for topic_idx,topic in enumerate(model.components_):\n",
    "        print('Topic #%d:'% topic_idx)\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "n_top_words = 10\n",
    "tf_feature_names = countvec.get_feature_names()\n",
    "print_top_words(ldamodel, tf_feature_names, n_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2.主题模型的gensim实现\n",
    "\n",
    "\n",
    "#文档预处理\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "filelist=[]\n",
    "for i in range(len(df2)):\n",
    "    filelist.append(df2['fenci'][i])\n",
    "#生成文档对应的字典和bow稀疏矩阵\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(filelist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in filelist] # 仍为list in list  \n",
    "\n",
    "tfidf_model = models.TfidfModel(corpus) # 建立TF-IDF模型  \n",
    "corpus_tfidf = tfidf_model[corpus] # 对所需文档计算TF-IDF结果\n",
    "corpus_tfidf\n",
    "\n",
    "#拟合LDA模型\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus, id2word = dictionary, num_topics = 3, passes = 10) \n",
    "#列出最重要的若干个主题\n",
    "ldamodel.print_topics(num_topics = 20,num_words = 10)\n",
    "\n",
    "#计算各语料的LDA模型值\n",
    "corpus_lda = ldamodel[corpus_tfidf] # 此处应当使用和模型训练时相同类型的矩阵\n",
    "for doc in corpus_lda:\n",
    "    print(doc)\n",
    "ldamodel.get_topics()#list of list 每个主题中每个词所对应的一个概率矩阵\n",
    "\n",
    "# 检索和文本内容最接近的主题\n",
    " # 检索和0.txt最接近的主题\n",
    "query_bow = dictionary.doc2bow(df2['fenci'][0]) # 频数向量\n",
    "query_tfidf = tfidf_model[query_bow] # TF-IDF向量\n",
    "print(\"转换后：\", query_tfidf[:10])\n",
    "ldamodel.get_document_topics(query_bow) # 需要输入和文档对应的bow向量\n",
    "# 检索和文本内容最接近的主题\n",
    "\n",
    "ldamodel[query_tfidf]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 对gensim的LDA结果作呈现\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)\n",
    "print('--------------')\n",
    "pyLDAvis.disable_notebook() # 关闭notebook支持后，可以看到背后所生成的数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.362 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.83 s\n",
      "[(2, 0.99918199)]\n",
      "[(2, 0.99918199)]\n",
      "[(1, 0.1047532), (11, 0.89464784)]\n",
      "[(6, 0.99924606)]\n",
      "[(1, 0.99903977)]\n",
      "[(8, 0.99853247)]\n",
      "[(3, 0.058429006), (10, 0.94022316)]\n",
      "[(11, 0.99900174)]\n",
      "[(3, 0.9983539)]\n",
      "[(11, 0.99971533)]\n",
      "[(3, 0.081012875), (6, 0.91799438)]\n",
      "[(14, 0.99831831)]\n",
      "[(0, 0.99973357)]\n",
      "[(8, 0.027190065), (13, 0.97153544)]\n",
      "[(8, 0.99817348)]\n",
      "[(13, 0.98962379)]\n",
      "[(13, 0.99911946)]\n",
      "[(7, 0.99809909)]\n",
      "[(8, 0.99834216)]\n",
      "[(0, 0.64452708), (3, 0.05509999), (6, 0.24650821), (11, 0.053195063)]\n",
      "[(2, 0.044811487), (3, 0.9541555)]\n",
      "[(10, 0.99783802)]\n",
      "[(1, 0.99942774)]\n",
      "[(6, 0.99861932)]\n",
      "[(3, 0.073396884), (11, 0.92589217)]\n",
      "[(10, 0.99948663)]\n",
      "[(3, 0.99930501)]\n",
      "[(13, 0.9996354)]\n",
      "[(9, 0.99971247)]\n",
      "[(3, 0.99934363)]\n",
      "[(6, 0.99966997)]\n"
     ]
    }
   ],
   "source": [
    "#用原始词频矩阵拟合LDA模型\n",
    "#2.主题模型的gensim实现\n",
    "\n",
    "\n",
    "#文档预处理\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "filelist=[]\n",
    "for i in range(len(df2)):\n",
    "    filelist.append(df2['fenci'][i])\n",
    "#生成文档对应的字典和bow稀疏矩阵\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(filelist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in filelist] # 仍为list in list  \n",
    "\n",
    "\n",
    "#拟合LDA模型\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus, id2word = dictionary, num_topics = 15, passes = 10) \n",
    "#列出最重要的若干个主题\n",
    "ldamodel.print_topics(num_topics = 15,num_words = 10)\n",
    "\n",
    "#计算各语料的LDA模型值\n",
    "corpus_lda = ldamodel[corpus] # 此处应当使用和模型训练时相同类型的矩阵\n",
    "for doc in corpus_lda:\n",
    "    print(doc)\n",
    "ldamodel.get_topics()#list of list 每个主题中每个词所对应的一个概率矩阵\n",
    "\n",
    "# 检索和文本内容最接近的主题\n",
    " # 检索和0.txt最接近的主题\n",
    "query_bow = dictionary.doc2bow(df2['fenci'][7]) # 频数向量\n",
    "ldamodel.get_document_topics(query_bow) # 需要输入和文档对应的bow向量\n",
    "# 检索和文本内容最接近的主题\n",
    "\n",
    "ldamodel[query_bow]\n",
    "ldamodel.save('C:/EdmsData/ldamodel.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.主题模型的sklearn实现\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#先弄出文档词频矩阵，然后进行相应的模型训练\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "countvec = CountVectorizer(min_df = 0.1)#最小词频\n",
    "X = countvec.fit_transform(rawfile)#稀疏矩阵\n",
    "\n",
    "#基于词频矩阵X计算TF-IDF值\n",
    "#---------------------------------\n",
    "#设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics=3#提取主题个数\n",
    "ldamodel = LatentDirichletAllocation(n_components= n_topics)\n",
    "#拟合LDA模型\n",
    "docres = ldamodel.fit_transform(X)\n",
    "#拟合后的模型实质\n",
    "print(docres)\n",
    "print(ldamodel.components_.shape)#(10,705)\n",
    "print(ldamodel.components_)#前两条，每个主题中每个词条出现的概率\n",
    "\n",
    "\n",
    "\n",
    "#打印主题词\n",
    "\n",
    "def print_top_words(model,feature_names,n_top_words):\n",
    "    for topic_idx,topic in enumerate(model.components_):\n",
    "        print('Topic #%d:'% topic_idx)\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "n_top_words = 10\n",
    "tf_feature_names = countvec.get_feature_names()\n",
    "print_top_words(ldamodel, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '4', '5', '6', '7', '8', '9']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 5, \n",
      "Perplexity Score 407.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 10, \n",
      "Perplexity Score 400.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 15, \n",
      "Perplexity Score 376.161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 20, \n",
      "Perplexity Score 380.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 25, \n",
      "Perplexity Score 400.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 30, \n",
      "Perplexity Score 399.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 35, \n",
      "Perplexity Score 389.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 40, \n",
      "Perplexity Score 404.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 45, \n",
      "Perplexity Score 406.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 50, \n",
      "Perplexity Score 403.194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 55, \n",
      "Perplexity Score 414.106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 60, \n",
      "Perplexity Score 421.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 65, \n",
      "Perplexity Score 425.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 70, \n",
      "Perplexity Score 428.017\n",
      "Best # of Topic:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.96512451e-05   9.96513868e-05   9.96513972e-05   9.96513158e-05\n",
      "    9.96512431e-05   9.96512332e-05   9.96512396e-05   9.98604882e-01\n",
      "    9.96513181e-05   9.96513638e-05   9.96513252e-05   9.96513749e-05\n",
      "    9.96512387e-05   9.96512409e-05   9.96512445e-05]\n",
      " [  9.96512451e-05   9.96513868e-05   9.96513972e-05   9.96513158e-05\n",
      "    9.96512431e-05   9.96512332e-05   9.96512396e-05   9.98604882e-01\n",
      "    9.96513181e-05   9.96513638e-05   9.96513252e-05   9.96513749e-05\n",
      "    9.96512387e-05   9.96512409e-05   9.96512445e-05]\n",
      " [  6.69344237e-05   6.69345399e-05   6.69345723e-05   6.69345293e-05\n",
      "    6.69344289e-05   6.69344139e-05   6.69344200e-05   6.69348245e-05\n",
      "    6.69345546e-05   9.99062917e-01   6.69345096e-05   6.69345335e-05\n",
      "    6.69344195e-05   6.69344262e-05   6.69344217e-05]\n",
      " [  7.99360728e-05   7.99362877e-05   7.99362331e-05   7.99361756e-05\n",
      "    7.99360816e-05   7.99360651e-05   7.99360716e-05   9.59607592e-01\n",
      "    7.99361950e-05   3.93532383e-02   7.99362226e-05   7.99362136e-05\n",
      "    7.99360694e-05   7.99360764e-05   7.99360776e-05]\n",
      " [  9.63391309e-05   9.63392567e-05   9.63392522e-05   9.98651251e-01\n",
      "    9.63391405e-05   9.63391214e-05   9.63391303e-05   9.63392906e-05\n",
      "    9.63392633e-05   9.63393086e-05   9.63392514e-05   9.63392798e-05\n",
      "    9.63391258e-05   9.63391342e-05   9.63391316e-05]\n",
      " [  1.66251135e-04   9.92078918e-01   1.66251500e-04   1.66251227e-04\n",
      "    1.66251072e-04   1.66251059e-04   1.66251072e-04   5.75981694e-03\n",
      "    1.66251250e-04   1.66251322e-04   1.66251301e-04   1.66251288e-04\n",
      "    1.66251073e-04   1.66251076e-04   1.66251097e-04]\n",
      " [  1.46843030e-04   1.46843299e-04   1.46843316e-04   1.46843161e-04\n",
      "    1.46843044e-04   1.46842905e-04   1.46842949e-04   1.46843315e-04\n",
      "    1.46843266e-04   9.97944196e-01   1.46843140e-04   1.46843363e-04\n",
      "    1.46842916e-04   1.46842932e-04   1.46842933e-04]\n",
      " [  1.08932486e-04   1.08932639e-04   9.56284842e-01   1.08932705e-04\n",
      "    1.08932510e-04   1.08932475e-04   1.08932487e-04   1.08932716e-04\n",
      "    1.08932721e-04   4.22990345e-02   1.08932636e-04   1.08932732e-04\n",
      "    1.08932485e-04   1.08932505e-04   1.08932488e-04]\n",
      " [  1.41242984e-04   1.41243194e-04   1.41243208e-04   1.41243291e-04\n",
      "    1.41242983e-04   1.41242956e-04   1.41242972e-04   9.98022597e-01\n",
      "    1.41243231e-04   1.41243353e-04   1.41243216e-04   1.41243174e-04\n",
      "    1.41242963e-04   1.41242977e-04   1.41242972e-04]\n",
      " [  3.79434765e-05   3.79435220e-05   3.79435359e-05   3.79435367e-05\n",
      "    3.79434789e-05   3.79434716e-05   3.79434746e-05   3.79435550e-05\n",
      "    8.29776551e-02   8.33655609e-01   3.79435368e-05   8.29114135e-02\n",
      "    3.79434738e-05   3.79434780e-05   3.79434760e-05]\n",
      " [  1.01936833e-04   1.82837304e-02   1.01936989e-04   1.01937061e-04\n",
      "    1.01936845e-04   1.01936819e-04   1.01936828e-04   4.97079313e-01\n",
      "    1.01936971e-04   4.83413714e-01   1.01937028e-04   1.01937049e-04\n",
      "    1.01936824e-04   1.01936831e-04   1.01936829e-04]\n",
      " [  2.15053810e-04   2.15054079e-04   9.96989244e-01   2.15054215e-04\n",
      "    2.15053858e-04   2.15053786e-04   2.15053808e-04   2.15054359e-04\n",
      "    2.15054120e-04   2.15054150e-04   2.15054038e-04   2.15054190e-04\n",
      "    2.15053797e-04   2.15053815e-04   2.15053810e-04]\n",
      " [  4.95294832e-05   4.95295674e-05   4.95295736e-05   4.95295475e-05\n",
      "    4.95294797e-05   4.95294768e-05   4.95294803e-05   4.95295843e-05\n",
      "    4.95295624e-05   4.95295780e-05   4.95295771e-05   9.99306587e-01\n",
      "    4.95294794e-05   4.95294839e-05   4.95294837e-05]\n",
      " [  1.44613219e-04   1.44613443e-04   1.61520257e-01   1.44613676e-04\n",
      "    1.44613241e-04   1.44613185e-04   1.44613197e-04   8.36599770e-01\n",
      "    1.44613550e-04   1.44613504e-04   1.44613394e-04   1.44613563e-04\n",
      "    1.44613221e-04   1.44613209e-04   1.44613208e-04]\n",
      " [  1.59490061e-04   1.59490084e-04   1.59490050e-04   1.59489995e-04\n",
      "    1.59489682e-04   1.59489663e-04   1.59489673e-04   9.97767142e-01\n",
      "    1.59489834e-04   1.59490099e-04   1.59489921e-04   1.59489897e-04\n",
      "    1.59489671e-04   1.59489685e-04   1.59489684e-04]\n",
      " [  1.75438649e-04   1.75438899e-04   9.97543857e-01   1.75438828e-04\n",
      "    1.75438637e-04   1.75438618e-04   1.75438631e-04   1.75439018e-04\n",
      "    1.75438901e-04   1.75438895e-04   1.75439096e-04   1.75438884e-04\n",
      "    1.75438629e-04   1.75438648e-04   1.75438639e-04]\n",
      " [  1.01471362e-04   1.01471540e-04   1.01471560e-04   1.01471519e-04\n",
      "    1.01471374e-04   1.01471357e-04   1.01471363e-04   8.24553251e-01\n",
      "    1.01471554e-04   1.01471628e-04   1.01471612e-04   1.74127620e-01\n",
      "    1.01471361e-04   1.01471371e-04   1.01471367e-04]\n",
      " [  2.58398045e-04   1.61032175e-01   2.58398782e-04   2.58398408e-04\n",
      "    2.58398081e-04   2.58397985e-04   2.58398047e-04   8.35608647e-01\n",
      "    2.58398511e-04   2.58398596e-04   2.58398390e-04   2.58398813e-04\n",
      "    2.58397997e-04   2.58398027e-04   2.58398034e-04]\n",
      " [  1.71379740e-04   1.71379954e-04   9.97600683e-01   1.71379843e-04\n",
      "    1.71379661e-04   1.71379629e-04   1.71379643e-04   1.71380072e-04\n",
      "    1.71379835e-04   1.71379952e-04   1.71379872e-04   1.71379892e-04\n",
      "    1.71379638e-04   1.71379642e-04   1.71379651e-04]\n",
      " [  9.55110123e-05   9.55111761e-05   9.55112081e-05   9.55111833e-05\n",
      "    9.55110310e-05   9.55110058e-05   9.55110203e-05   4.88165268e-01\n",
      "    9.55111363e-05   5.10593088e-01   9.55111151e-05   9.55112076e-05\n",
      "    9.55110111e-05   9.55110140e-05   9.55110111e-05]\n",
      " [  1.45878962e-04   1.45879267e-04   1.45879263e-04   1.45879129e-04\n",
      "    1.45878967e-04   1.45878949e-04   1.45878958e-04   9.97957693e-01\n",
      "    1.45879183e-04   1.45879173e-04   1.45879176e-04   1.45879280e-04\n",
      "    1.45878954e-04   1.45878971e-04   1.45878975e-04]\n",
      " [  5.79710321e-05   5.79711055e-05   5.79711307e-05   5.79710911e-05\n",
      "    5.79710368e-05   5.79710247e-05   5.79710292e-05   5.79711552e-05\n",
      "    5.79711709e-05   9.99188405e-01   5.79711839e-05   5.79711396e-05\n",
      "    5.79710273e-05   5.79710349e-05   5.79710376e-05]\n",
      " [  8.65801076e-05   8.65802471e-05   8.65803221e-05   8.65802144e-05\n",
      "    8.65801263e-05   8.65800999e-05   8.65801068e-05   8.65802820e-05\n",
      "    8.65802551e-05   9.98787877e-01   8.65802113e-05   8.65804327e-05\n",
      "    8.65801052e-05   8.65801222e-05   8.65801124e-05]\n",
      " [  1.32013246e-04   1.32013555e-04   1.32013429e-04   1.32013472e-04\n",
      "    1.32013231e-04   1.32013219e-04   1.32013233e-04   9.98151813e-01\n",
      "    1.32013352e-04   1.32013515e-04   1.32013466e-04   1.32013404e-04\n",
      "    1.32013225e-04   1.32013231e-04   1.32013244e-04]\n",
      " [  9.60615101e-05   9.60616414e-05   9.60616867e-05   9.60616838e-05\n",
      "    9.60615323e-05   9.60614979e-05   9.60615056e-05   9.60618031e-05\n",
      "    9.60617188e-05   9.98655138e-01   9.60616361e-05   9.60617081e-05\n",
      "    9.60615057e-05   9.60615185e-05   9.60615074e-05]\n",
      " [  7.38279954e-05   7.38280963e-05   7.38280865e-05   7.38280939e-05\n",
      "    7.38279997e-05   7.38279868e-05   7.38279929e-05   7.38281178e-05\n",
      "    7.38280931e-05   7.38282329e-05   9.98966407e-01   7.38280943e-05\n",
      "    7.38279897e-05   7.38279963e-05   7.38279952e-05]\n",
      " [  7.19166146e-05   1.61812165e-01   7.19167195e-05   7.19167226e-05\n",
      "    7.19166110e-05   7.19165898e-05   7.19165960e-05   2.80577894e-01\n",
      "    7.19166791e-05   5.56746941e-01   7.19167461e-05   7.19167473e-05\n",
      "    7.19165935e-05   7.19165962e-05   7.19165959e-05]\n",
      " [  4.99375875e-05   4.99376453e-05   4.99376629e-05   4.99376819e-05\n",
      "    4.99376202e-05   4.99375845e-05   4.99375912e-05   4.99376666e-05\n",
      "    4.99376764e-05   4.99376979e-05   4.99376538e-05   9.99300873e-01\n",
      "    4.99375870e-05   4.99375929e-05   4.99375891e-05]\n",
      " [  4.22475783e-05   4.22476299e-05   4.22476436e-05   4.22476235e-05\n",
      "    4.22475838e-05   4.22475750e-05   4.22475786e-05   4.22476440e-05\n",
      "    9.99408533e-01   4.22477526e-05   4.22476335e-05   4.22476461e-05\n",
      "    4.22475773e-05   4.22475831e-05   4.22475795e-05]\n",
      " [  7.32600903e-05   9.98974358e-01   7.32602124e-05   7.32602404e-05\n",
      "    7.32600951e-05   7.32600821e-05   7.32600893e-05   7.32602433e-05\n",
      "    7.32602182e-05   7.32602719e-05   7.32602067e-05   7.32602252e-05\n",
      "    7.32600861e-05   7.32600953e-05   7.32600897e-05]\n",
      " [  3.83803594e-05   3.83804070e-05   3.83804384e-05   3.83804211e-05\n",
      "    3.83803690e-05   3.83803566e-05   3.83803616e-05   9.99462674e-01\n",
      "    3.83804539e-05   3.83804592e-05   3.83804276e-05   3.83804575e-05\n",
      "    3.83803596e-05   3.83803644e-05   3.83803610e-05]]\n",
      "(15, 933)\n",
      "[[ 0.37476155  0.26295248  0.44889998 ...,  0.25657244  0.24840982\n",
      "   0.41927649]\n",
      " [ 1.10482809  0.25433363  1.15900709 ...,  0.26590992  0.26039173\n",
      "   0.32710528]\n",
      " [ 1.20227772  0.24378451  0.24157252 ...,  0.72573804  0.27643443\n",
      "   0.26896746]\n",
      " ..., \n",
      " [ 0.27386697  0.23377134  0.22897641 ...,  0.20370006  0.28209865\n",
      "   0.23396237]\n",
      " [ 0.23913601  0.23488872  0.2563663  ...,  0.24331805  0.2424247\n",
      "   0.23589404]\n",
      " [ 0.27579245  0.2803665   0.2860553  ...,  0.29517816  0.2515909\n",
      "   0.25999868]]\n",
      "Topic #0:\n",
      "软件工程 技术 数据挖掘 数据 信息 学生 发展趋势 带来 优势 发展\n",
      "Topic #1:\n",
      "数据 企业 客户 技术 信息 计算机网络 发展 网络 网络安全 消费\n",
      "Topic #2:\n",
      "技术 计算机 设计 样本 图像处理 图像 内容 算法 发展 数据\n",
      "Topic #3:\n",
      "数据 数据挖掘 医疗 技术 信息 算法 挖掘 分析 模式 对象\n",
      "Topic #4:\n",
      "区域 功能 基础 识别 特征 模型 向量 密度 研究 数据\n",
      "Topic #5:\n",
      "云计算 计算 兴趣 项目 发现 挖掘 部门 职业 分割 模型\n",
      "Topic #6:\n",
      "数据 文本 区域 过程 定位 基础 挖掘 研究 机器学习 模型\n",
      "Topic #7:\n",
      "数据 技术 文本 数据挖掘 分析 系统 缺陷 方法 学生 信息\n",
      "Topic #8:\n",
      "模型 图像 算法 样本 质量 方法 提升 预测 随机 判定\n",
      "Topic #9:\n",
      "数据 数据挖掘 算法 分析 研究 信息 方法 技术 学生 服务\n",
      "Topic #10:\n",
      "影响 本文 分析 公司 因素 数据 决策树 预测 创新 预测模型\n",
      "Topic #11:\n",
      "模型 区域 时间 数据 功能 调度 研究 基础 识别 运营\n",
      "Topic #12:\n",
      "学习 深度 技术 文本 数据 发展 领域 算法 分析 研究\n",
      "Topic #13:\n",
      "样本 算法 图像 模型 质量 提升 技术 特征 采用 内容\n",
      "Topic #14:\n",
      "数据 技术 学生 网络 信息 发展 计算机 网络安全 研究 课程\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.主题模型的sklearn实现\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#先弄出文档词频矩阵，然后进行相应的模型训练\n",
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "rawfile=[]#空格分隔开的文本\n",
    "for i in range(len(df2)):\n",
    "    rawfile.append(' '.join(df2['fenci'][i]))\n",
    "#print(rawfile) \n",
    "#rawfile listoflist ,每一项为用空格连接的一篇文档\n",
    "countvec = CountVectorizer(min_df = 0.1)#最小词频\n",
    "X = countvec.fit_transform(rawfile)#稀疏矩阵\n",
    "\n",
    "#基于词频矩阵X计算TF-IDF值\n",
    "#---------------------------------\n",
    "#设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = range(5, 75, 5)\n",
    "perplexityLst = [1.0]*len(n_topics)\n",
    " \n",
    "#训练LDA并打印训练时间\n",
    "lda_models = []\n",
    "for idx, n_topic in enumerate(n_topics):\n",
    "    lda = LatentDirichletAllocation(n_topics=n_topic,max_iter=20,learning_method='batch',evaluate_every=200,verbose=0)\n",
    "  \n",
    "    lda.fit(X)\n",
    "    perplexityLst[idx] = lda.perplexity(X)\n",
    "    lda_models.append(lda)\n",
    "    print(\"# of Topic: %d, \" % n_topics[idx])\n",
    "\n",
    "    print(\"Perplexity Score %0.3f\" % perplexityLst[idx])\n",
    "#打印最佳模型\n",
    "best_index = perplexityLst.index(min(perplexityLst))\n",
    "best_n_topic = n_topics[best_index]\n",
    "best_model = lda_models[best_index]\n",
    "print(\"Best # of Topic: \", best_n_topic)\n",
    "\n",
    "#设定LDA模型\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics=best_n_topic#提取主题个数\n",
    "ldamodel = LatentDirichletAllocation(n_components= n_topics)\n",
    "#拟合LDA模型\n",
    "docres = ldamodel.fit_transform(X)\n",
    "#拟合后的模型实质\n",
    "print(docres)\n",
    "print(ldamodel.components_.shape)#(10,705)\n",
    "print(ldamodel.components_)#前两条，每个主题中每个词条出现的概率\n",
    "\n",
    "\n",
    "\n",
    "#打印主题词\n",
    "\n",
    "def print_top_words(model,feature_names,n_top_words):\n",
    "    for topic_idx,topic in enumerate(model.components_):\n",
    "        print('Topic #%d:'% topic_idx)\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "n_top_words = 10\n",
    "tf_feature_names = countvec.get_feature_names()\n",
    "print_top_words(ldamodel, tf_feature_names, n_top_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
