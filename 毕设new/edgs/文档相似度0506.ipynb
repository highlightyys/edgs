{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.037 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Tools\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Topic: 5, \n",
      "Perplexity Score -8.174\n",
      "# of Topic: 10, \n",
      "Perplexity Score -8.033\n",
      "# of Topic: 15, \n",
      "Perplexity Score -8.041\n",
      "# of Topic: 20, \n",
      "Perplexity Score -7.998\n",
      "# of Topic: 25, \n",
      "Perplexity Score -7.889\n",
      "# of Topic: 30, \n",
      "Perplexity Score -7.936\n",
      "# of Topic: 35, \n",
      "Perplexity Score -7.991\n",
      "# of Topic: 40, \n",
      "Perplexity Score -7.912\n",
      "# of Topic: 45, \n",
      "Perplexity Score -7.907\n",
      "# of Topic: 50, \n",
      "Perplexity Score -7.953\n",
      "# of Topic: 55, \n",
      "Perplexity Score -7.961\n",
      "# of Topic: 60, \n",
      "Perplexity Score -7.995\n",
      "# of Topic: 65, \n",
      "Perplexity Score -7.964\n",
      "# of Topic: 70, \n",
      "Perplexity Score -8.066\n",
      "Best # of Topic:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 列出所消耗的时间备查\\n%time ldamodel = LdaModel(corpus, id2word = dictionary, num_topics = 30, passes = 10) #拟合LDA模型\\nprint(ldamodel.print_topics(num_topics = 30,num_words = 10))\\n\\n# 计算各语料的LDA模型值\\ncorpus_lda = ldamodel[corpus] # 此处应当使用和模型训练时相同类型的矩阵\\nfor doc in corpus_lda:\\n    print(doc)\\n\\nfrom gensim import similarities\\nsimmtx = similarities.MatrixSimilarity(corpus) # 使用的矩阵种类需要和拟合模型时相同\\nsimmtx.index[:2]\\n\\n# 使用gensim的LDA拟合结果进行演示\\nquery_bow = dictionary.doc2bow(df2.fenci[40])\\nprint(df2.filename[0])\\nlda_vec = ldamodel[query_bow] # 转换为lda模型下的向量\\nsims = simmtx[lda_vec] # 进行矩阵内向量和所提供向量的余弦相似度查询\\nsims = sorted(enumerate(sims), key=lambda item: -item[1])\\nsims\\n\\nfor i in range(len(sims)):\\n    print(sims[i])\\n    a,b = sims[i]\\n    if b != 0.0:\\n        print(df.filename[a])\\n        \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from MyDataFrame import MyDataFrame\n",
    "mdf = MyDataFrame()\n",
    "df = mdf.new_DataFrame()\n",
    "df2 = mdf.m_cut(df)\n",
    "filelist=[]\n",
    "for i in range(len(df2)):\n",
    "    filelist.append(df2['fenci'][i])\n",
    "#生成文档对应的字典和bow稀疏矩阵\n",
    "from gensim import corpora, models  \n",
    "\n",
    "dictionary = corpora.Dictionary(filelist)  \n",
    "corpus = [dictionary.doc2bow(text) for text in filelist]\n",
    "\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "#----------------------------------test----------------------------\n",
    "n_topics = range(5, 75, 5)\n",
    "perplexityLst = [1.0]*len(n_topics)\n",
    "#训练LDA并打印训练时间\n",
    "lda_models = []\n",
    "for idx, n_topic in enumerate(n_topics):\n",
    "    #lda = LatentDirichletAllocation(n_topics=n_topic,max_iter=20,learning_method='batch',evaluate_every=200,verbose=0)\n",
    "    lda = LdaModel(corpus, id2word = dictionary, num_topics = n_topic, passes = 10)\n",
    "    lda[corpus]\n",
    "    \n",
    "    perplexityLst[idx] = lda.log_perplexity(corpus)\n",
    "    \n",
    "    lda_models.append(lda)\n",
    "    print(\"# of Topic: %d, \" % n_topics[idx])\n",
    "    print(\"Perplexity Score %0.3f\" % perplexityLst[idx])\n",
    "#打印最佳模型\n",
    "best_index = perplexityLst.index(min(perplexityLst))\n",
    "best_n_topic = n_topics[best_index]\n",
    "best_model = lda_models[best_index]\n",
    "print(\"Best # of Topic: \", best_n_topic)\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "# 列出所消耗的时间备查\n",
    "%time ldamodel = LdaModel(corpus, id2word = dictionary, num_topics = 30, passes = 10) #拟合LDA模型\n",
    "print(ldamodel.print_topics(num_topics = 30,num_words = 10))\n",
    "\n",
    "# 计算各语料的LDA模型值\n",
    "corpus_lda = ldamodel[corpus] # 此处应当使用和模型训练时相同类型的矩阵\n",
    "for doc in corpus_lda:\n",
    "    print(doc)\n",
    "\n",
    "from gensim import similarities\n",
    "simmtx = similarities.MatrixSimilarity(corpus) # 使用的矩阵种类需要和拟合模型时相同\n",
    "simmtx.index[:2]\n",
    "\n",
    "# 使用gensim的LDA拟合结果进行演示\n",
    "query_bow = dictionary.doc2bow(df2.fenci[40])\n",
    "print(df2.filename[0])\n",
    "lda_vec = ldamodel[query_bow] # 转换为lda模型下的向量\n",
    "sims = simmtx[lda_vec] # 进行矩阵内向量和所提供向量的余弦相似度查询\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "sims\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    print(sims[i])\n",
    "    a,b = sims[i]\n",
    "    if b != 0.0:\n",
    "        print(df.filename[a])\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 16:54:23,084 : INFO : adding document #0 to Dictionary(0 unique tokens: []) : \n",
      "2019-05-06 16:54:23,185 : INFO : built Dictionary(15144 unique tokens: ['0.01', '0.05', '0002', '00100.0095', '00105.00']...) from 50 documents (total 73016 corpus positions) : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the info of this ldamodel: \n",
      "\n",
      "num of testset: 1; size_dictionary: 15144; num of topics: 10\n",
      "the perplexity of this ldamodel is : 39.007824767774764\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf-8')\n",
    "import os\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim import corpora, models\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s : ', level=logging.INFO)\n",
    " \n",
    "def perplexity(ldamodel, testset, dictionary, size_dictionary, num_topics):\n",
    "    \"\"\"calculate the perplexity of a lda-model\"\"\"\n",
    "    # dictionary : {7822:'deferment', 1841:'circuitry',19202:'fabianism'...]\n",
    "    print ('the info of this ldamodel: \\n')\n",
    "    print ('num of testset: %s; size_dictionary: %s; num of topics: %s'%(len(testset), size_dictionary, num_topics))\n",
    "    prep = 0.0\n",
    "    prob_doc_sum = 0.0\n",
    "    topic_word_list = [] # store the probablity of topic-word:[(u'business', 0.010020942661849608),(u'family', 0.0088027946271537413)...]\n",
    "    for topic_id in range(num_topics):\n",
    "        topic_word = ldamodel.show_topic(topic_id, size_dictionary)\n",
    "        dic = {}\n",
    "        for word, probability in topic_word:\n",
    "            dic[word] = probability\n",
    "        topic_word_list.append(dic)\n",
    "    doc_topics_ist = [] #store the doc-topic tuples:[(0, 0.0006211180124223594),(1, 0.0006211180124223594),...]\n",
    "    for doc in testset:\n",
    "        doc_topics_ist.append(ldamodel.get_document_topics(doc, minimum_probability=0))\n",
    "    testset_word_num = 0\n",
    "    for i in range(len(testset)):\n",
    "        prob_doc = 0.0 # the probablity of the doc\n",
    "        doc = testset[i]\n",
    "        doc_word_num = 0 # the num of words in the doc\n",
    "        for word_id, num in doc:\n",
    "            prob_word = 0.0 # the probablity of the word \n",
    "            doc_word_num += num\n",
    "            word = dictionary[word_id]\n",
    "            for topic_id in range(num_topics):\n",
    "                # cal p(w) : p(w) = sumz(p(z)*p(w|z))\n",
    "                prob_topic = doc_topics_ist[i][topic_id][1]\n",
    "                prob_topic_word = topic_word_list[topic_id][word]\n",
    "                prob_word += prob_topic*prob_topic_word\n",
    "            prob_doc += math.log(prob_word) # p(d) = sum(log(p(w)))\n",
    "        prob_doc_sum += prob_doc\n",
    "        testset_word_num += doc_word_num\n",
    "    prep = math.exp(-prob_doc_sum/testset_word_num) # perplexity = exp(-sum(p(d)/sum(Nd))\n",
    "    print (\"the perplexity of this ldamodel is : %s\"%prep)\n",
    "    return prep\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    from MyDataFrame import MyDataFrame\n",
    "    mdf = MyDataFrame()\n",
    "    df = mdf.new_DataFrame()\n",
    "    df2 = mdf.m_cut(df)\n",
    "    filelist=[]\n",
    "    for i in range(len(df2)):\n",
    "        filelist.append(df2['fenci'][i])\n",
    "#生成文档对应的字典和bow稀疏矩阵\n",
    "    from gensim import corpora, models  \n",
    "\n",
    "    dictionary = corpora.Dictionary(filelist)  \n",
    "    corpus = [dictionary.doc2bow(text) for text in filelist]\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    lda_multi = joblib.load('ldamodel_0506_10.model')\n",
    "    \n",
    "    \n",
    "    num_topics = 10\n",
    "    testset = []\n",
    "    # sample 1/300\n",
    "    for i in range(int(len(df)/50)):\n",
    "        testset.append(corpus[i*50])\n",
    "        \n",
    "    prep = perplexity(lda_multi, testset, dictionary, len(dictionary.keys()), num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
